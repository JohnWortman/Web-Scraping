{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The url can easily be modified to loop over multiple years where the corresponding scraped data can be easily read into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the webpage in a chrome browser\n",
    "# path to chromedrive is unique\n",
    "driver = webdriver.Chrome(executable_path=r'C:\\Users\\johnw\\Desktop\\Python\\Chromedriver\\chromedriver_win32\\chromedriver')\n",
    "url = 'https://fortune.com/fortune500/2019/search/'\n",
    "source = driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow some time for the webpage to load\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ad pops up, select \"No, Thanks\" on the sign-up screen\n",
    "try:\n",
    "    driver.find_element_by_xpath('//*[@id=\"bx-element-1186146-JWp4eVy\"]/button').click()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting page length: 100 Rows\n"
     ]
    }
   ],
   "source": [
    "# our function to expand the page to 100 companies        \n",
    "# row options are in the 4th <select> tag\n",
    "# loop through all the options and see if they are \"100 Rows\"\n",
    "# if True, click the element\n",
    "rows = driver.find_elements_by_tag_name('select')\n",
    "rows_100 = rows[4].find_elements_by_tag_name('option')\n",
    "for row in rows_100:\n",
    "    if row.text == '100 Rows':\n",
    "        print('Selecting page length: ' + row.text)\n",
    "        row.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rank', 'name', 'rev_MM', 'rev_pct_change', 'profit_MM', 'profit_pct_change', 'assets_MM', 'mkt_value_MM', 'rank_change_1000', 'employees', 'rank_change_500']) \n",
      "\n",
      "[('1', 'Walmart'), ('2', 'Exxon Mobil'), ('3', 'Apple'), ('4', 'Berkshire Hathaway'), ('5', 'Amazon.com'), ('6', 'UnitedHealth Group'), ('7', 'McKesson'), ('8', 'CVS Health'), ('9', 'AT&T'), ('10', 'AmerisourceBergen')]\n"
     ]
    }
   ],
   "source": [
    "# fields to populate with webscrape\n",
    "\n",
    "# keys for all_data dictionary\n",
    "catagories = [\n",
    "    'rev_MM',\n",
    "    'rev_pct_change',\n",
    "    'profit_MM',\n",
    "    'profit_pct_change',\n",
    "    'assets_MM',\n",
    "    'mkt_value_MM',\n",
    "    'rank_change_1000',\n",
    "    'employees',\n",
    "    'rank_change_500',\n",
    "]\n",
    "\n",
    "# data to be collected\n",
    "data = []\n",
    "rank = []\n",
    "name = []\n",
    "\n",
    "# define the function that will be used to scrape the website\n",
    "def scrape():\n",
    "    # load in the html for each page\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # define pattern for regex matching data\n",
    "    pattern = re.compile('[\\$-*\\d{,3},*\\d{1,3}.\\d]*')\n",
    "\n",
    "    # scrape table data from webpage\n",
    "    # loop through all table rows <tr> and table data <td>\n",
    "    divs = soup.find_all('div', class_ = 'rt-tr-group')   \n",
    "    for div in divs:\n",
    "        # regex to pull company rank and name\n",
    "        match = re.findall('\\d+', div.text)\n",
    "        rank.append(match[0])\n",
    "        name.append(div.text.split(match[0])[1].split('$')[0])\n",
    "        \n",
    "        # find all our table data <td> values\n",
    "        rows = div.findAll('div', class_='rt-td searchResults__cell--2Y7Ce')\n",
    "        \n",
    "        # check if table data <td> values match our regex, if so pull the data\n",
    "        for row in rows:\n",
    "            if re.match(pattern, row.text):\n",
    "                data.append(row.text)\n",
    "        \n",
    "\n",
    "\n",
    "# loops through 5 pages to get the fortune 500 data\n",
    "# find button --> scrape data --> click button\n",
    "def run_scrape():\n",
    "    for i in range(0, 5):\n",
    "        for button in driver.find_elements_by_class_name('-next'):\n",
    "            if button.text == 'NEXT':\n",
    "                scrape()\n",
    "                button.click()\n",
    "\n",
    "    # close the browser when finished with the session            \n",
    "    driver.close()           \n",
    "            \n",
    "\n",
    "    \n",
    "# populate dictionary with scraped data using logic\n",
    "all_data = {}\n",
    "def format_scrape():\n",
    "    all_data['rank'] = rank\n",
    "    all_data['name'] = name\n",
    "    for i in range(0, 9):\n",
    "        # grab every 9th element starting at element i\n",
    "        # and place it into the \"ith\" catagory \n",
    "        all_data[catagories[i]] = data[i::9]\n",
    "    return all_data\n",
    "        \n",
    "            \n",
    "# display our data  \n",
    "run_scrape()\n",
    "format_scrape()\n",
    "print(all_data.keys(), '\\n')\n",
    "print(list(zip(all_data['rank'][:10], all_data['name'][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load our data into a pandas datafram for diaplay\n",
    "df = pd.DataFrame(all_data).set_index('rank')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/DataFrame.PNG \"Title\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
